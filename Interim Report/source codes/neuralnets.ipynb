{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from random import sample\n",
    "import utils\n",
    "from utils import shuffling\n",
    "from utils import normalization\n",
    "from utils import traintestsplit\n",
    "from utils import feature_extract\n",
    "from utils import feature_extract\n",
    "from utils import initializer\n",
    "from utils import encode_label\n",
    "from utils import maxminnorm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"bonn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset==\"bonn\":\n",
    "\n",
    "        epilepsy_data = pd.read_csv(\"bonn_epilepsy.csv\", sep =\",\")\n",
    "        epilepsy_data.drop(\"Unnamed\",axis=1,inplace=True)\n",
    "        epilepsy_data.head()\n",
    "        epilepsy_data.y = epilepsy_data.y==1\n",
    "        epilepsy_data.y = epilepsy_data.y.astype(int)\n",
    "\n",
    "        epilepsy_data= epilepsy_data[epilepsy_data.isnull().any(axis=1)==False]\n",
    "\n",
    "        label = epilepsy_data[\"y\"].astype(\"category\").to_numpy()\n",
    "        label2 = epilepsy_data[\"y\"]\n",
    "      \n",
    "        epilepsy_data.drop(\"y\",axis=1,inplace=True)\n",
    "        epilepsy_data= feature_extract(epilepsy_data)\n",
    "        epilepsy_data= epilepsy_data.iloc[:,-13:]\n",
    "        #normalize the data\n",
    "        normalized = normalization(epilepsy_data, label2)\n",
    "elif dataset==\"beirute\":\n",
    "    epilepsy_data = pd.read_csv(\"beirut_epilepsy.csv\", sep =\",\")\n",
    "    epilepsy_data.drop(\"Unnamed\",axis=1,inplace=True)\n",
    "\n",
    "    epilepsy_data = epilepsy_data.loc[(epilepsy_data[\"y\"] == 0) | (epilepsy_data[\"y\"] == 1)].reset_index()\n",
    "    epilepsy_data.drop(\"index\",axis=1,inplace=True)\n",
    "\n",
    "    epilepsy_data.y = epilepsy_data.y.astype(int)\n",
    "    epilepsy_data= epilepsy_data[epilepsy_data.isnull().any(axis=1)==False]\n",
    "\n",
    "    label = epilepsy_data[\"y\"].astype(\"category\").to_numpy()\n",
    "    label2 = epilepsy_data[\"y\"]\n",
    "    epilepsy_data.drop(\"y\",axis=1,inplace=True)\n",
    "\n",
    "    epilepsy_data = feature_extract(epilepsy_data)\n",
    "\n",
    "    epilepsy_data= epilepsy_data.iloc[:,-5:]\n",
    "    #normalize the data\n",
    "    normalized = maxminnorm(epilepsy_data, label2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the data\n",
    "shuffled = shuffling(normalized)\n",
    "#split the data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = traintestsplit(shuffled,0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9775, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_data= X_train\n",
    "np_test_data= X_test\n",
    "\n",
    "np_label_train=Y_train\n",
    "np_label_test=Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_plot(true_label,predictions):\n",
    "    \n",
    "\n",
    "    class_labels = np.unique(true_label,return_counts=False)\n",
    "    \n",
    "    cm= confusion_matrix(true_label,predictions) \n",
    "    cp=ConfusionMatrixDisplay(cm,display_labels=class_labels)\n",
    "    cp.plot()\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self,currNeurons, activFunction,name):\n",
    "        self.currNeurons=currNeurons\n",
    "        self.activFunction=activFunction\n",
    "        self.name=name\n",
    "        \n",
    "        self.delta=None\n",
    "        self.error=None\n",
    "        self.cacheActiv=None\n",
    "        self.weights=None\n",
    "    \n",
    "    def layer_activation(self,net):\n",
    "        if self.activFunction==\"relu\":\n",
    "            return np.maximum(0,net)\n",
    "        elif self.activFunction==\"sigmoid\":\n",
    "            return 1/(1+np.exp(-net))\n",
    "        elif self.activFunction==\"tanh\":\n",
    "            return np.tanh(net)\n",
    "        elif self.activFunction==\"softmax\":\n",
    "            return np.exp(net)/np.sum(np.exp(net),axis=0)\n",
    "    \n",
    "    def layer_derivative(self,activ):\n",
    "        \n",
    "        if self.activFunction==\"relu\":\n",
    "            return 1*(activ>0)\n",
    "        elif self.activFunction==\"sigmoid\":\n",
    "            return activ*(1-activ)\n",
    "        elif self.activFunction==\"tanh\":\n",
    "            return 1-activ**2\n",
    "        elif self.activFunction==\"softmax\":\n",
    "               return activ*(1-activ)\n",
    "            \n",
    "      \n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \n",
    "    def __init__(self,inputDim):\n",
    "        self.layers=[]\n",
    "        self.inputDim=inputDim\n",
    "        \n",
    "        self.loss=None\n",
    "        self.lr=0.01\n",
    "        \n",
    "        \n",
    "    \n",
    "    def layer_addition(self,layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def compile_(self,lossFn,learning_rate,weight_initializer):\n",
    "        \n",
    "        if lossFn==\"mse\":\n",
    "            self.loss=\"mse\"\n",
    "        elif lossFn==\"cross_entropy\":\n",
    "            self.loss=\"cross_entropy\"\n",
    "        \n",
    "        self.lr=learning_rate\n",
    "        self.weight_initializer= weight_initializer\n",
    "           \n",
    "        for i in range(len(self.layers)):\n",
    "          \n",
    "            lyr=self.layers[i]\n",
    "            if i==0:\n",
    "                lyr.weights= initializer(init_choice=self.weight_initializer,shape=(self.layers[0].currNeurons,self.inputDim+1))\n",
    "            else:\n",
    "                lyr.weights= initializer(init_choice=self.weight_initializer,shape=(self.layers[i].currNeurons,self.layers[i-1].currNeurons+1))\n",
    "        \n",
    "    def forward_propagation(self,inp):\n",
    "        \n",
    "        output=inp\n",
    "     \n",
    "        for lyr in self.layers:\n",
    "           \n",
    "            num_samples=output.shape[1]\n",
    "            output=np.r_[output,[np.ones(num_samples)*1]]\n",
    "            output=lyr.layer_activation(np.matmul(lyr.weights,output))\n",
    "           \n",
    "            lyr.cacheActiv=output\n",
    "         \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def predict(self, data):\n",
    "        out = self.forward_propagation(data)\n",
    "        return np.argmax(out, axis=0) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def backward_propagation(self,inp,out):\n",
    "        batch_size= inp.shape[1]\n",
    "        network_output=self.forward_propagation(inp)\n",
    "        \n",
    "        for ly in reversed(range(len(self.layers))):\n",
    "\n",
    "                layer= self.layers[ly]\n",
    "                if layer == self.layers[len(self.layers)-1]:\n",
    "                    \n",
    "                    if self.loss==\"cross_entropy\" and layer.activFunction==\"softmax\":\n",
    "                        layer.delta=network_output-out\n",
    "                     \n",
    "                    \n",
    "                    elif self.loss==\"mse\":\n",
    "                        \n",
    "                        layer.error = network_output-out\n",
    "                        errorDerivative = layer.layer_derivative(layer.cacheActiv)\n",
    "                        layer.delta=layer.error*errorDerivative\n",
    "                    else:\n",
    "                        print(\"Sorry, I do not want to perform this operation.\")\n",
    "                \n",
    "                else:\n",
    "                        layerNext = self.layers[ly+1]\n",
    "                        layerNextWeights= layerNext.weights[:,:-1]\n",
    "                        layer.error = np.matmul(layerNextWeights.T,layerNext.delta)\n",
    "                        errorDerivative= layer.layer_derivative(layer.cacheActiv)\n",
    "                        layer.delta=errorDerivative*layer.error\n",
    "                        \n",
    "        \n",
    "        for ly in range(len(self.layers)):\n",
    "          \n",
    "            if ly==0:\n",
    "            \n",
    "                activationInput=np.r_[inp,[np.ones(batch_size)*1]]\n",
    "          \n",
    "            else:\n",
    "                activationInput=np.r_[self.layers[ly-1].cacheActiv,[np.ones(batch_size)*1]]\n",
    "            \n",
    "            \n",
    "            dW= np.matmul(self.layers[ly].delta,activationInput.T)/batch_size\n",
    "            \n",
    "            self.layers[ly].weights=self.layers[ly].weights-self.lr*dW\n",
    "            \n",
    "            \n",
    "    def loss_calculation(self,data,label):\n",
    "        \n",
    "            val_out = self.forward_propagation(data.T)\n",
    "            val_label=label\n",
    "            if(self.loss == 'cross_entropy'):\n",
    "                err = - np.sum(np.log(val_out)*encode_label(val_label).T)/val_out.shape[1]\n",
    "            elif(self.loss == 'mse'):\n",
    "                err = np.sum((encode_label(val_label).T - val_out)**2)/val_out.shape[1]\n",
    "            return err\n",
    "                    \n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self,data,label,val_ratio,epoch,batch_size):\n",
    "        trainError= []\n",
    "        trainAcc=[]\n",
    "        valError=[]\n",
    "        valAcc=[]\n",
    "        \n",
    "        \n",
    "        for e in range(epoch):\n",
    "        \n",
    "        \n",
    "           \n",
    "            print('=====================================================\\nEpoch', e+1)\n",
    "            indx= np.random.permutation(data.shape[0])\n",
    "            data_temp=data[indx]\n",
    "            label_temp=label[indx]\n",
    "            \n",
    "            val_range=int(data.shape[0]*val_ratio)\n",
    "           \n",
    "            val_data= data[:val_range]\n",
    "          \n",
    "            val_label=label[:val_range]\n",
    "            \n",
    "            data_temp=data_temp[val_range:]\n",
    "            label_temp=label_temp[val_range:]\n",
    "            \n",
    "            iterations = int(np.floor(len(data)/batch_size))\n",
    "            \n",
    "            for it in range(iterations):\n",
    "                \n",
    "                data_batch=data_temp[it*batch_size:it*batch_size+batch_size]\n",
    "                label_batch=label_temp[it*batch_size:it*batch_size+batch_size]\n",
    "                if label_batch.shape[0]!=0:\n",
    "                    label_batch = encode_label(label_batch)            \n",
    "                    self.backward_propagation(data_batch.T, label_batch.T)\n",
    "                \n",
    "            err_tr= self.loss_calculation(data_temp,label_temp)\n",
    "            err_val=self.loss_calculation(val_data,val_label)\n",
    "            valError.append(err_val)\n",
    "            trainError.append(err_tr)\n",
    "            if self.loss==\"cross_entropy\":\n",
    "                print(\"CE loss in training: \", err_tr)\n",
    "                print(\"CE loss in training: \", err_val)\n",
    "            else:\n",
    "                print(\"MSE loss in training: \", err_tr)\n",
    "                print(\"MSE loss in training: \", err_val)\n",
    "                \n",
    "\n",
    "           \n",
    "                        \n",
    "            pred_train = self.predict(data.T)\n",
    "        \n",
    "            #trAcc = np.sum(trPred.reshape((trPred.shape[0],1)) == out)/trPred.shape[0]*100\n",
    "            acc_train=np.sum(pred_train==label)/pred_train.shape[0]\n",
    "            print('Training Accuracy: ', acc_train*100)\n",
    "            trainAcc.append(acc_train*100)\n",
    "            \n",
    "            pred_val = self.predict(val_data.T)\n",
    "            acc_val= np.sum(pred_val==val_label)/pred_val.shape[0]\n",
    "            print('Validation Accuracy: ', acc_val*100)\n",
    "            valAcc.append(acc_val*100)\n",
    "            \n",
    "        \n",
    "        return trainAcc, valAcc, valError,trainError\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, create your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp= Sequential(X_train.shape[1]) \n",
    "mlp.layer_addition(Layer(50,\"tanh\",\"layer1\"))\n",
    "mlp.layer_addition(Layer(20,\"tanh\",\"layer2\"))\n",
    "mlp.layer_addition(Layer(10,\"tanh\",\"layer3\"))\n",
    "mlp.layer_addition(Layer(2,\"softmax\",\"layer4\"))\n",
    "mlp.compile_(\"cross_entropy\",0.5,\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Epoch 1\n",
      "CE loss in training:  0.08748847737629761\n",
      "CE loss in training:  0.08293063051606027\n",
      "Training Accuracy:  96.51150895140665\n",
      "Validation Accuracy:  96.98209718670077\n",
      "=====================================================\n",
      "Epoch 2\n",
      "CE loss in training:  0.08603435122423124\n",
      "CE loss in training:  0.07952862290456779\n",
      "Training Accuracy:  96.46035805626599\n",
      "Validation Accuracy:  96.82864450127877\n",
      "=====================================================\n",
      "Epoch 3\n",
      "CE loss in training:  0.08612219371049314\n",
      "CE loss in training:  0.0777338252391605\n",
      "Training Accuracy:  96.65473145780051\n",
      "Validation Accuracy:  97.03324808184142\n",
      "=====================================================\n",
      "Epoch 4\n",
      "CE loss in training:  0.08648514661879485\n",
      "CE loss in training:  0.07764951460591224\n",
      "Training Accuracy:  96.66496163682864\n",
      "Validation Accuracy:  97.2378516624041\n",
      "=====================================================\n",
      "Epoch 5\n",
      "CE loss in training:  0.09151691325057969\n",
      "CE loss in training:  0.07931874486334914\n",
      "Training Accuracy:  96.27621483375958\n",
      "Validation Accuracy:  96.62404092071611\n"
     ]
    }
   ],
   "source": [
    "trainAcc, valAcc, valError,trainError = mlp.fit(np_train_data,np_label_train,0.2,5,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.05797101449275\n"
     ]
    }
   ],
   "source": [
    "testPred = mlp.predict(np_test_data.T)\n",
    "testAcc = np.sum(testPred ==np_label_test)/testPred.shape[0]\n",
    "print(testAcc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
